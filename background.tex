\chapter{Background}

\section{GPU Architecture}
GPUs are highly parallel architectures that aim to always have cores running some useful piece of code. GPUs can be thought of as a CPU with dozens of cores and quick context switching. As a core stalls for a memory read or write, another warp (analogous to a lightweight process) is scheduled to that core. If the hardware has enough work to schedule across all cores, maximum throughput is achieved and memory latency can be effectively hidden. This GPU model is highly efficient at parallelizing regular code patterns.

Warps can be broken down even further into threads. Each thread represents a Single Instruction Multiple Data (SIMD) lane. The GPU will have dozens of cores running code in parallel, and each individual core will be running 32 or more instances of the same code as a thread. SIMD greatly increases the amount of parallelization within the hardware, but, as the name suggests, allows a core to only run a single instruction at a time across its threads. Warps rely on their threads to exhibit regular code and data access patterns. Memory stalls and divergent branch paths within threads both negatively impact performance, effectively serializing code execution in the worst case. Dynamic warp formation \cite{dynamicwarp1,dynamicwarp2} and memory coalescing can offset the effects of code and data divergence respectively, but these techniques cannot completely account for the negative effects of poor code design and inefficient memory access patterns.

Each core on the GPU has a register file, an L1 cache, and on chip SRAM referred to as shared memory. All L1 caches are backed by a shared L2 cache, which in turn is backed by DRAM or global memory. Shared memory can be thought of as per-process memory, it is fast and solely owned by the warp that allocates it. Global memory is the only way for different warps to communicate. This memory hierarchy helps to alleviate latency of memory accesses. An access to the L1 or L2 cache is less taxing on warps than waiting on a memory access to DRAM.

\section{Cache Eviction and Replacement}
Cache replacement policies are a highly researched and developed area in computer architecture. Least Recently Used (LRU) is typically used as a baseline metric in papers because it is easy to reason about and gives relatively good performance for its simplicity \cite{lruperf}. However, LRU only learns on basic information currently within the cache (there are no extra data stores for previously evicted lines). Additionally, LRU is susceptible to poor performance given pathological access patterns that can cause thrashing. Early attempts to improve LRU such as Qureshi’s et al’s DIP paper \cite{dip} avoid the effects of thrashing by augmenting LRU to occasionally insert lines into the Most Recently Used (MRU) position. Over the past couple decades we have seen a trend from heuristic based approaches, such as DIP, to more theoretically grounded replacement policies like EVA \cite{eva} and Hawkeye \cite{hawkeye}.

Belady’s OPT algorithm \cite{belady_opt} is the optimal cache replacement policy assuming no prefetching, identical cost for all misses, and knowledge of the future. When determining what line to evict, OPT chooses the line that will be reused furthest in the future. Belady’s OPT can be used as a ground truth for algorithms attempting to predict optimal cache replacement choices. In the related work section we will briefly elaborate on how Hawkeye calculates OPT online with the OPTGen algorithm, and we will discuss how OPTGen informs Hawkeye’s replacement decisions.

\section{Deadblock Predictors}
Deadblock predictors \cite{deadblock} are a natural evolution of basic replacement policies like LRU. Deadblock predictors opt to predict what lines will no longer be reused again in the cache rather than using locally optimal decisions like LRU. Deadblock predictors predict on a variety of features such as timestamps of hits and PC of the hit. We find deadblock predictors interesting for the idea of precleaning because we believe they can be used to also help us predict a write that benefits from precleaning. Predicting a deadblock is similar to predicting when a line will not see writes again in the future. We hope to learn from and alter deadblock predictors like Cache Burst \cite{cache_burst} and EVA \cite{eva} to predict when a modified line will not be written to again before eviction.
