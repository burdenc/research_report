\chapter{Background}
\index{Background@\emph{Background}}

\section{GPU Architecture}
\index{GPU Architecture@\emph{GPU Architecture}}
GPUs are highly parallel architectures that aim to always have cores running some useful piece of code. GPUs can be thought of as a CPU with dozens of cores and quick context switching. As a core stalls for a memory read or write, another warp (analogous to a lightweight process) is scheduled to that core. In this way if the hardware has enough work to schedule across all cores, maximum throughput is achieved and memory latency can be effectively hidden. This GPU model is highly efficient at parallelizing regular code patterns.

Warps can be broken down even further into threads. Each thread represents a Single Instruction Multiple Data (SIMD) lane [cite?]. The GPU will have dozens of cores running code in parallel, and each individual core will be running 32 or more instances of the same code. SIMD greatly increases the amount of parallelization within the hardware, but, as the name suggests, allows a core to only run a single instruction at a time across its threads. Warps rely on their threads to exhibit regular code and data access patterns. Memory stalls and divergent branch paths within threads both negatively impact performance, effectively serializing parts of the warp. Dynamic warp formation \cite{dynamicwarp1,dynamicwarp2} and memory coalescing [cite?] can help combat code and data divergence respectively, but these techniques cannot completely account for the negative effects of poor code design and inefficient memory access patterns.

Each core on the GPU has a relatively large register file, an L1 cache, and on chip SRAM referred to as shared memory. All L1 caches are backed by a shared L2 cache, which in turn is backed by DRAM or global memory. Shared memory can be thought of as per-process memory, it is fast and solely owned by the warp that allocates it. Global memory is the only way for different warps to communicate. This memory hierarchy helps to alleviate latency of memory accesses. An access to the L1 or L2 cache is much less taxing on warps than waiting on a memory access to DRAM.

\section{Cache Eviction and Replacement}
\index{Cache Eviction and Replacement@\emph{Cache Eviction and Replacement}}
Cache replacement policies are a highly researched and developed area in computer architecture. Least Recently Used (LRU) is often used as a baseline metric in papers because it is easy to reason about and gives relatively good performance for its simplicity \cite{lruperf}. However, LRU only learns on basic information currently within the cache (there are no extra data stores for previously evicted lines). Additionally, LRU is susceptible to poor performance given pathological access patterns that can cause thrashing. Early attempts to improve LRU such as Qureshi’s et al’s DIP paper \cite{dip} attempt avoid the effects of thrashing by augmenting LRU to randomly insert lines into the Most Recently Used (MRU) position. Over the past couple decades we have seen a trend from heuristic based approaches such as DIP to more theoretically grounded replacement policies like EVA \cite{eva} and Hawkeye.

Belady’s OPT algorithm \cite{belady_opt} is the optimal cache replacement policy assuming no prefetching, identical cost for all misses, and knowledge of the future. When determining what line to evict, OPT chooses the line that will be reused furthest in the future. Belady’s OPT can be used as a ground truth for machine learning algorithms attempting to predict optimal cache replacement choices. In the related work section we will briefly elaborate on how Hawkeye calculates OPT online with the OPTGen algorithm, and we will discuss how OPTGen informs Hawkeye’s replacement decisions.

\section{Deadblock Predictors}
\index{Deadblock Predictors@\emph{Deadblock Predictors}}
Deadblock predictors \cite{deadblock} are a natural evolution of basic replacement policies like LRU. Deadblock predictors opt to predict what lines will no longer be reused again in the cache rather than using locally greedy decisions like LRU. Deadblock predictors predict on a variety of features such as timestamps of hits, PC of the hit, etc. In particular we will be looking at non-PC based deadblock predictors, as typically LLCs on GPUs don’t have access to PC information (and we’re currently not confident that across so many warps PC would give much information anyways). Deadblock predictors that forego PC information include Cache Burst \cite{cache_burst} and EVA \cite{eva}. We find deadblock predictors interesting for the idea of precleaning because we believe they can be used to also help us predict a write that benefits from precleaning.
