\chapter{Solution}

In this section we cover the specifics of both applying Hawkeye to GPU caching and a cache precleaning system. Our approach for Hawkeye consists of fine tuning and optimizing the predictor for a GPU environment and evaluating the importance of replacement policies on GPUs. Additionally, we present the basic building blocks required for a cache precleaning system such as a modified deadblock predictor and a bandwidth usage prediction scheme.

\section{Hawkeye on GPU}
Adapting Hawkeye to GPUs requires proper training features and adapting caching mechanisms to support said features. We implement Hawkeye with two basic features: the Program Counter (PC) and PC combined with warp ID (PC+WID). Furthermore, we implement and evaluate Hawkeye on both the L1 and L2 cache.

The PC is a rich feature to train on in CPUs and is a common feature in CPU cache replacement policies, prefetchers, and branch predictors. However, kernels on GPUs tend to be much smaller and have fewer accesses to global memory compared to processes on CPUs. These factors lead to a much smaller number of PCs that access global memory presumably making it harder to use only PC as a feature for Hawkeye. To alleviate this lack of diversity in PC values, we consider warp ID as an additional feature. If we combine PC with the ID of the warp executing the memory access we can achieve a more fine grained set of values to train our predictor on. We use Cantor pairing to act as a hashing mechanism between PC and warp ID. While Cantor pairing gives decent performance for how simple it is, we believe Cantor pairing may be inefficient in hardware because of the multiplication step involved. Refer to Figure \ref{f:cantor} for a definition of the Cantor pairing function.

\begin{figure}[htb]
\begin{center}
$$
Cantor(x, y) = \frac{(x + y) (x + y + 1)}{2} + y
$$
\caption{Cantor pairing function. For the purposes of Hawkeye we use the Cantor pairing function as a rudimentary hashing function between PC and warp ID.}
\label{f:cantor}
\end{center}
\end{figure}

Typically, on CPUs, complex cache replacement policies are reserved for last level caches where high latency is expected and more memory is available for storing meta-data. However, as weâ€™ve seen in implementations like APCM, L1 caches on GPUs can actually have complex replacement policies while still seeing performance benefits. GPU on-chip memory is shared between the register file, shared memory, and L1 cache and can be used to store meta-data for the L1 cache replacement policies. The additional meta-data required to implement Hawkeye can come both from the L1 cache itself or additional memory can be repurposed from the register file and shared memory if needed. Furthermore, because GPUs are inherently dependent on throughput and not latency, we don't expect slight increases in latency at the L1 level to have a significant, negative impact on performance.

\section{Cache Precleaning}

To successfully hide write traffic, a proper solution requires a prediction of: the final write to a line, and the optimal time to commit precleans to lower level memory. Given a trace of the program from previous executions, one could perfectly reorder writes to minimize the impact write backs have on the GPU's memory systems. Our first prediction, determining the final write to a line, considers all lines that are dirty and requires information such as the history of writes to said line and the line's reuse interval. Our second prediction, the optimal time to commit precleans, involves looking at current and past bandwidth usage and using markers in the code and the warp scheduler.

\subsection{Final Write Prediction}
Deadblock predictors are useful for precleaning because they give us an idea of how active a line currently is. It's likely that a deadblock is a good candidate for precleaning, because said line is predicted to have no reads or writes in the future. Though we would like to be more precise than this, we would like a mechanism to predict when a line is a deadblock for the purposes of writes but is still being accessed by reads. Potentially important features for this predictor include the history of writes to this line, the global history of writes, and which lines will be evicted soon. We believe we can modify a deadblock predictor to predict precleaning candidates by finding when the final write for a line is executed. Currently, this is one of the crucial, unexplored areas for the problem of precleaning. For our experiments we have a basic system using LRU as the indicator for when a line should be precleaned.

\subsection{Bandwidth Variation and Prediction}
A common code pattern in GPU programs is to load data in from global memory into shared memory, do parallel computations, then write back the result of the computations to global memory. Because of the nature of parallel programming, there are often times when different computation units need to synchronize and wait for all other units before continuing. A convenient time to do this synchronization is right after cores write back to global memory. This synchronization ensures that all threads see the latest state of computation before continuing. Please refer to Figure \ref{f:cuda_sync} for basic pseudo-code that shows this coding pattern.

\begin{figure}[htb]
\begin{center}

\begin{lstlisting}
for (int i = 0; i < N; i++) {
  int new_data_idx = getNextPieceOfData(threadIdx);
  int* data = global_data[new_data_idx];
  doComputations(data);
  global_data[new_data_idx] = *data;
  syncThreads();
}
\end{lstlisting}
\caption{Pseudo-code for common synchronization code pattern in CUDA code. This data is loaded in from DRAM (as this is the only way data can be shared across cores on a GPU) to local shared memory. When computations are done and stored back to global memory, we need to manually synchronize to prevent race conditions in the data.}
\label{f:cuda_sync}
\end{center}
\end{figure}
\index{commands!environments!figure}%

%Figure 2: Here we load in data assigned to our current thread (which may or may not change as the program goes on). This data is loaded in from DRAM (as this is the only way data can be shared across cores on a GPU) to local shared memory. When computations are done and stored back to global memory, we need to manually synchronize to prevent race conditions in the data.

This synchronization technique yields a useful bandwidth pattern for our technique of precleaning. As cores hit the synchronization barrier their bandwidth usage will drop off, making the synchronization point an opportune time to send out write backs. By getting hints from the compiler or looking at the state of a warp a predictor can infer when threads are attempting to synchronize. We also see much more pronounced dips in bandwidth usage between executions of kernels, though kernel termination happens much less often compared to thread synchronization. Again, bandwidth usage prediction still requires more research, for our experiments we usage a basic heuristic based on the number of outstanding memory accesses to DRAM.
