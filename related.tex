\chapter{Related Work}
\index{Related Work@\emph{Related Work}}

This report draws upon many different concepts in computer architecture cited above, but the three related works we find the most relevant are Hawkeye’s original publication by Jain and Lin \cite{hawkeye}, Koo et al’s Access Pattern-Aware Cache Management (APCM) paper \cite{apcm}, and two patents by Marvell and Nvidia that are similar to our idea of precleaning \cite{preclean_cpu,preclean_nvidia_patent}.

\section{Hawkeye}
Hawkeye, as touched on in the background section, uses OPTGen as an online method for calculating the optimal cache replacement policy. OPTGen is always trying to match two sequential accesses to a cache line over the program's execution. The time between these sequential accesses represents how long a line needs to be in the cache before getting a hit. With a 4-way associative cache OPTGen can only have 4 of these lines overlapping before a line must be evicted or bypassed. OPTGen, like Belady’s OPT, always prefers lines that has its next access the earliest, and thus prefers caching lines with the shortest interval between accesses. When OPTGen decides to forego caching a line, this line’s associated feature is trained negatively (cache unfriendly), and OPTGen positively trains (cache friendly) features associated with lines that are successfully cached. When Hawkeye needs to evict a cache line it prefers to evict the lines that OPTGen determines to be cache unfriendly. When all lines are cache friendly, Hawkeye falls back on RRIP \cite{rrip} to decide which line to evict. Unfortunately, achieving 100\% accurate calculations of Belady's OPT would require infinite memory. The original Hawkeye paper shows that OPTGen can get within 99\% accuracy of OPT when using cache sampling and restricting the size of the access history vector.

\section{APCM}
APCM is a paper on GPU cache management improvements that our research draws inspiration from for its cache sensitivity study and its application to L1 caches. APCM provides key insights into the differences between CPU and GPU caches. APCM detects when lines are likely to exhibit reuse between warps of the same core. The cache management policy then pins or bypasses lines to maximize reuse and avoid thrashing within the L1 cache.

To evaluate headroom, APCM measures how sensitive certain GPU benchmarks are to cache performance through its cache sensitivity study. Furthermore, APCM also introduces the idea of sampling warps for its replacement policy meta-data. Rather than adding costly sampling mechanisms to all warps, APCM instead assumes that GPU programs are regular enough such that a single warp's access patterns is representative of all the other warps. Finally, APCM is notable for affirming the idea that more complicated cache replacement policies can be applied at the L1 without significant performance costs. Typically, on CPUs, we would not see cache replacement policies more complicated than LRU replacement at the L1 level due to latency and cost concerns. However, APCM shows that we can benefit from a more complex replacement policy at the L1 level on GPUs.

\section{Precleaning Related Patents}
In our research we found two patents that describe or come close to our idea of precleaning. The patent by Marvell \cite{preclean_cpu} gives a high level idea of how a precleaning unit would work, but unfortunately gives us no insight into the problem due to the lack of details on prediction and precleaning criteria. The patent filed by Nvidia \cite{preclean_nvidia_patent} doesn't quite cover our idea of precleaning, but does attempt to optimize away the costs of writebacks by using an intermediate cache. From our research we don't believe there is any published prior work that has attempted to implement what we've described as precleaning in this paper. It still remains to be seen if precleaning can provide performance benefits on GPU caches.
