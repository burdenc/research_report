\chapter{Related Work}

This report draws upon many different concepts in computer architecture cited above, but the three related works we find the most relevant are Hawkeye’s original publication by Jain and Lin \cite{hawkeye}, Koo et al’s Access Pattern-Aware Cache Management (APCM) paper \cite{apcm}, and two patents by Marvell and Nvidia that are similar to our proposed idea of precleaning \cite{preclean_cpu,preclean_nvidia_patent}.

\section{Hawkeye}
Hawkeye uses OPTGen as an online method for calculating the optimal cache replacement policy. At every cache access, OPTGen tries to match two sequential accesses to a cache line over the program's execution. The time between these sequential accesses represents how long a line needs to be in the cache before getting a hit. For example, a 4-way set associative cache can only have 4 of these cache access intervals overlapping before a line must be evicted or bypassed. OPTGen, like Belady’s OPT, always prefers lines that has its next access the earliest. When OPTGen sees a new access, it looks back through the history of all cache accesses to find the last access to this address. By looking back through the history of acceses, OPTGen construct the exact interval that this line needs to sit in the cache to be a hit. If adding this interval would exceed the associativity of the cache, the line would not be cached by Belady's OPT. Unfortunately, achieving 100\% accurate calculations of Belady's OPT would require infinite memory. The original Hawkeye paper shows that OPTGen can get within 99\% accuracy of OPT when using cache sampling and restricting the size of the access history vector.

When OPTGen decides to forego caching a line, Hawkeye negatively trains this line’s associated feature negatively (cache unfriendly). Additionally, Hawkeye positively trains (cache friendly) features associated with lines that are successfully cached by OPTGen. When Hawkeye needs to evict a cache line it prefers to evict the lines that were trained as cache unfriendly. When all lines are cache friendly, Hawkeye falls back on RRIP to decide which line to evict \cite{rrip}. 

\section{APCM}
APCM is a paper on GPU cache management improvements that our research draws inspiration from for its cache sensitivity study and its application to L1 caches. APCM provides key insights into the differences between CPU and GPU caches. APCM detects when lines are likely to exhibit reuse between warps of the same core. The cache management policy then pins or bypasses lines to maximize reuse and avoid thrashing within the L1 cache.

To evaluate headroom, APCM measures how sensitive certain GPU benchmarks are to cache performance through its cache sensitivity study. Furthermore, APCM also introduces the idea of sampling warps for its replacement policy meta-data. Rather than adding costly sampling mechanisms to all warps, APCM instead assumes that, because most warps are executing identical code, a single warp's access patterns is representative of all the other warps on the core. This gives us insight into what types of features will be useful for Hawkeye to predict on. Finally, APCM is notable for affirming the idea that more complicated cache replacement policies can be applied at the L1 without significant performance costs. Typically, on CPUs, we would not see cache replacement policies besides LRU replacement at the L1 level due to latency and cost concerns. However, APCM shows that we can benefit from a more complex replacement policy at the L1 level on GPUs.

\section{Precleaning Related Patents}
In our research we found two patents that come close to describing or fully describe our idea of precleaning. The patent by Marvell \cite{preclean_cpu} gives a high level idea of how a precleaning unit works, but unfortunately gives us no insight into the problem due to the lack of details on prediction and precleaning criteria. The patent filed by Nvidia \cite{preclean_nvidia_patent} doesn't quite cover our idea of precleaning, but does attempt to optimize away the costs of writebacks by using an intermediate cache. From our research we don't believe there is any published prior work that has attempted to implement what we've described as precleaning in this paper. It still remains to be seen if precleaning can provide performance benefits on GPU caches.
