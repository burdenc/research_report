\chapter{Introduction}

GPUs cater to highly parallel and regular data access patterns. While GPUs provide hardware specifically tailored for parallel problems, good GPU performance relies on programs exhibiting behavior that can be partitioned into thousands of highly regular threads executing the same code. Warps are what we call a collection of threads that run in lock-step, and all threads within a warp should run similar code and access relatively nearby data in memory. Data divergence occurs when these warps overload lower-level memory systems by exhibiting irregular access patterns and accessing non-consectutive cache lines. When a program starts to exhibit high levels of data divergence, cache optimizations become more important for retaining good performance. Our goal in this paper, with both Hawkeye and precleaning, is to limit and offset the negative performance impacts of both data divergence and poorly optimized data access patterns. 

In this report, we explore two methods for improving performance of GPU memory systems: Hawkeye cache replacement and cache precleaning. We implement Hawkeye, a cache replacement algorithm that learns from a delayed computation of the optimal cache replacement policy \cite{hawkeye}. The second method, which we refer to as precleaning, avoids memory system stalls by committing write-backs at a time of low bandwidth usage rather than at the time of eviction.

\section{Hawkeye}
Computer architects have been using caches and smart cache replacement policies to hide the latency of memory operations for decades \cite{deadblock,lruperf,cache_burst,dip,eva,rrip}. Caches ideally stores all data that will be accessed by the machine in the near future, reducing latency of memory accesses on the critical path. Unfortunately, cache memory is expensive and limited, thus architects rely on cache replacement policies to decide what data will be the most useful in the future.

Least Recently Used (LRU) is the baseline replacement policy for most caches as it is easy to reason about and performs well considering its relative simplicity. LRU does not store any information on lines previously evicted from the cache. Because of this lack of additional meta-data, LRU generally doesn't work well with access patterns with working sets that exceed the size of the cache. A cache with a more sophisticated replacement policy that can handle irregular or complex data access patterns can filter out some of the requests that would otherwise overload the GPU's main memory. By providing a proven approach to cache replacement on CPUs, we hope to improve performance on GPU programs that would previously see no benefit from the cache due to said programs’ data access patterns.

Our first idea for improving GPU memory system performance is to use Hawkeye, a CPU cache replacement policy that learns from the theoretically optimal cache replacement decisions for previous accesses to the cache. Hawkeye prefers to evict lines similar to other lower performing lines in the OPTGen algorithm, an online calculation of Belady’s optimal cache replacement policy. Hawkeye evaluates similarities between lines by comparing their defining features, such as warp ID and Program Counter (PC). Features help us correlate performance within OPTGen with future performance of lines with identical features. We explore the application of Hawkeye in both the L1 and L2 caches.

\section{Precleaning}
Precleaning, our second idea for improving memory system performance, aims to spread out the bandwidth usage by writing back dirty cache lines at times of lower bandwidth usage. Write-through and write-back caches are the two main ways that modern caches handle mutable data within the cache. Write-through caches immediately write modified data back to their backing store (such as a lower level cache or DRAM). On the other hand, write-back caches delay committing writes to their backing store until the associated line is evicted. This delay in committing writes is possible because non-atomic writes are off the critical path, meaning the processor will never need to stall for a non-atomic write. Write-back caches tend to be more efficient than write-through caches as write-through caches require an access to lower level memory for every single write. While the strategy of writing back at the last possible moment might be the most convenient, it is not necessarily the best time for data to be written back to lower level memory. Writing back at the time of eviction can be especially problematic in GPUs when unoptimized parallel code causes many simultaneous evictions. The key takeaway is that, rather than a binary choice of write-through and write-back, we actually have a full spectrum of choices to choose from when cleaning dirty cache lines.

For example, a GPU program that needs to load in large chunks of new data at once will incur many evictions in the cache. These evictions will cause write-backs, all clustered around the same time, saturating our bandwidth and stalling the machine. Referring to Figure \ref{f:bandwidth_optimal} we see that we could make better use of our bandwidth by writing back our dirty lines earlier, rather than at eviction.


We believe this problem is exaggerated on GPUs due to the parallel nature of the hardware. Making accesses to memory across many cores increases the chances of multiple write-backs being triggered around the same time. GPUs rely on having adequate bandwidth available to hide latency across all cores. When bandwidth saturates, code executing on the GPU will effectively start to serialize, which negates the performance benefits of using a GPU. 

\begin{figure}[htb]
\begin{center}
\ \psfig{file=figs/bandwidth_optimal.png,width=\textwidth}
\caption{Example of cache cleaning choices, showing potential improvement for bandwidth usage. Traditionally, the two options available were at the time of the original write (i.e. a write through cache) or at time of the eviction (i.e. a write back cache). In this example both write through and write back are arbitrary when trying to optimize for bandwidth usage. We can reason that the better choice is when bandwidth usage is minimal.}
\label{f:bandwidth_optimal}
\end{center}
\end{figure}
\index{commands!environments!figure}%

To further improve GPU memory system performance, we would like to use previous data and current memory bandwidth usage levels to determine a better time to write back modified cache lines. A perfect solution would evenly spread out all write-back traffic, preventing bandwidth spikes that could stall our GPU cores.
