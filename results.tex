\chapter{Experiments and Headroom}
\index{Experiments and Headroom@\emph{Experiments and Headroom}}

For our experiments we used GPGPUsim with a variety of programs from the Rodinia 2.0 \cite{rodinia} and Lonestar 2.0 \cite{lonestar} benchmark suites. We chose these programs to see a spread in both regular and irregular data access patterns across our experiments. Later in this section, we present promising results for applying Hawkeye to our GPU simulator environment. However, due to the lack of both time and domain knowledge, we were only able to gather basic headroom data for our idea of cache precleaning. In the following sections we will describe the details of the simulator used, discuss properties of our benchmarks and platform such as cache sensitivity and bandwidth usage, and finally present headroom and actual performance improvements using Hawkeye and precleaning.

\section{Simulator}
We use the GPGPUsim \cite{gpgpusim} simulator for our experiments, targeting the provided configuration that approximates the Nvidia Fermi architecture. This architecture supports 15 cores, each core consisting of 32 SIMD lanes and a maximum of 48 scheduled warps. Each core has a private 16 KB, 4-way set associative L1 cache with 128 bytes lines, and the entire system has a shared 786 KB, 8-way set associative L2 cache with 128 byte lines. The L2 cache is split into 12 portions and assigned to the 12 separate DRAM memory partitions. Finally, the L1 cache acts as write-through cache while the L2 cache acts as a write-back cache.

GPGPUsim runs CPU code natively, then intercepts CUDA library calls in order to perform full functional simulation (as opposed to a trace-based simulation). Because GPGPUsim doesn’t provide an option for trace-based simulation, we cannot skip warmup cycles and simulations take significant amount of time to run to completion. Due to these restrictions, simulations were sometimes cut short and compared against a baseline that ran for the same amount of time. Unfortunately, this means that our results are often extrapolated and could be inaccurate.

The version of GPGPUsim we use is a slight modification of the 3.0 release that has basic support for CUDA 5.5. This modified version is used in order to be able to run the Lonestar 2.0 benchmark suite. Furthermore, we made use of the AerialVision \cite{aerialvision} visualization software provided with GPGPUsim, but we made minor modifications in order to analyze bandwidth patterns while researching precleaning.

The baseline caches within the simulator are all managed by a standard implementation of LRU (no pseudo timestamps are used). Furthermore, as is common on GPUs, the caches have no prefetching and they only cache global reads/writes (accesses to shared memory are on chip and are thus equivalent to an L1 access). All atomic operations skip the L1 cache, and writes cause evictions from the L1 cache.

\section{Cache Sensitivity}
Cache sensitivity tells us how much cache performance affects a given benchmark. We would expect that a benchmark with heavy code divergence or a small memory footprint would see little to no improvements even from an optimal cache implementation. By enlarging the cache and measuring the change in instructions per clock (IPC) over the baseline, we can get a rough approximation of how much a given benchmark relies on the cache. If we see little to no improvement when enlarging the cache, we wouldn't expect a change in replacement policy to have much affect on performance either. Benchmarks with large IPC improvements have high cache sensitivity and benchmarks that do roughly the same as baseline are considered to have low cache sensitivity. In our Hawkeye experiments, we would like to see large performance improvements on benchmarks that have high cache sensitivity, but see performance on par with LRU in benchmarks with low cache sensitivity. This experiment is motivated by the same study done by the APCM paper as referenced in our Related Work section earlier.

To measure L1 cache sensitivity, we quadruple the size of the L1 cache and compare IPC performance against our baseline cache described above. We also repeat this experiment for the L2 cache. Our results for this experiment are presented in Figure \ref{f:sensitivity} in Appendix A. Interestingly enough, we see that a number of graph algorithms including MST, SSSP, and BFS exhibit high levels of cache sensitivity. Graph algorithms on GPU tend to do poorly due to their irregular and hard to predict memory access patterns. The other interesting thing to note here is that between two runs of bfs we see different levels of cache sensitivity, this could be due to how the two data sets were generated or the differing sizes of the data sets. We also see a performance drop in some benchmarks when enlarging the size of the cache. We believe this strange result can be explained by Belady’s anomaly \cite{belady_anomaly}.

\section{Hawkeye}
For Hawkeye on GPU, we first evaluate both PC and PC combined with warp ID as training features by comparing prediction biases. Next, we evaluate real performance impact by measuring the change in miss rate and IPC over the LRU baseline. Overall, we find that Hawkeye does well compared to LRU, and that, despite our initial thoughts, PC provides better performance than the PC+WID feature at the L1 cache level. Though the best feature to use at the L2 level isn't as obvious a choice.

To begin, we measure the per-feature bias of the OPTGen algorithm for both the PC and PC+WID features. Per-feature bias essentially tells us the prediction quality of our chosen feature, it tells us how often predictions on a feature agree with each other. We note that the HOTSPOT benchmark has a 100\% per-feature bias. We believe this high bias is caused by a well optimized code base that provides a small amount of PC values (a max of 4 in our simulations). The full set of per-feature biases for L1 and L2 caches can be seen in Figures \ref{f:l1_bias} and \ref{f:l2_bias} respectively.

\begin{figure}[htb]
\begin{center}
\ \psfig{file=figs/l1_ipc.png,width=\textwidth}
\caption{IPC improvements over baseline when applying Hawkeye to L1 caches. Improvements are measured with both PC and PC+WID features and also with default (4-way associate) and fully associative OPTgen training. FA denotes the fully-associative OPTgen training.}
\label{f:l1_ipc}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\ \psfig{file=figs/l2_ipc.png,width=\textwidth}
\caption{IPC improvements over baseline when applying Hawkeye to L2 caches.}
\label{f:l2_ipc}
\end{center}
\end{figure}

An important difference between the PC and PC+WID features that's not displayed in our bias metric is the larger number of values being trained on for PC+WID. In Figure \ref{f:opt_uniq_vals} is a plot of the number of unique values seen per feature. This large increase in potential feature values requires adequate hardware to track, and more values being trained typically leads to slower training times. In Figure \ref{f:cpu_opt_uniq_vals} we present the number of unique PC values seen by OPTGen when running on a last level cache on a CPU using SPEC 2006 \cite{spec} benchmarks and ChampSim \cite{champsim}. We note that the number of unique values seen for PC+WID on GPU and PC on CPU are both on the same order of magnitude. We believe there is more room for complex feature experimentation, especially involving dynamic warp IDs or better hashing algorithms.

In our full performance tests we measure two important metrics: post-warmup cache miss rate, and IPC improvement over baseline. For the post-warmup miss rate metric, we ignore all cache misses before a specified warmup time, as most misses at the beginning of a program are compulsory and cannot be avoided. We find that a post-warmup time of 3,000,000 cycles fits most of the benchmarks being used (with the exception of HOTSPOT which ends before 3,000,000 cycles). We present the miss rate improvements over LRU for L1 and L2 caches in Figures \ref{f:l1_miss_rate} and \ref{f:l2_miss_rate}. As mentioned above, PC performs better than the PC+WID feature for all L1 caching benchmarks. We present IPC improvements for L1 and L2 caches in Figures \ref{f:l1_ipc} and \ref{f:l2_ipc}. IPC improvements appear to roughly correlate with our sensitivity metric from before, for instance KMEANS does the best in both the sensitivity study and in our L1 IPC measurements. However, LUD, which exhibited high sensitivity saw almost no difference from applying Hawkeye at L1.

The performance gains seen at the L1 level are encouraging, as we see significant performance improvements on a few benchmarks. Hawkeye applied to L2 caches gives less significant results, but this is mostly in line with what we found in our cache sensitivity study above. It appears that PC is currently the best feature to use with Hawkeye on GPUs at both the L1 and L2 level. Furthermore, we believe that training OPTgen on a fully-associative cache isn't a good choice as we consistently see performance drops compared to the default OPTGen.

Overall, we see that naively applying Hawkeye to L1 caches gives decent performance improvements. While it is unclear if the cost of Hawkeye at the L1 level can be justified, we believe a simplified, fine-tuned implementation of Hawkeye can give significant performance improvements for a subset of GPU problems. However, Hawkeye applied at the L2 cache currently gives inconclusive results. On average we see little to no performance improvements and even large dips in performance depending on the benchmark. From these results, we believe the focus of more complex cache replacement should be put towards the L1 cache rather than the L2 cache.

\section{Precleaning}

The results for our idea of precleaning include a coarse measure of headroom and performance tests of the heuristic based predictor we tried. The headroom tests give a idealized view of how much room we have to improve. Like our Hawkeye results, we measure performance benefits in IPC improvement over baseline with no precleaning enabled.

For our headroom study we measure performance improvement when assuming no cost for executing a write-back. In an ideal scenario, a precleaner would be able to completely hide the effects of write-back being sent to main memory. This experiment also gives us an idea of which benchmarks are bandwidth sensitive, much like our cache sensitivity study earlier. We refer to Figure \ref{f:preclean_headroom} for the results of the headroom study. Rodinia's BFS sees a large increase in performance due to the extra bandwidth available. Rodinia's BFS is a clear outlier from the other benchmarks, we hypothesize that this is caused by a underutilization of shared memory causing all writes to go through global memory. If our hypothesis is correct, this would lead us to believe that precleaning has the most potential improvements on unoptimized, ported code that does not properly make use of CUDA specific concepts like shared memory.

\begin{figure}[htb]
\begin{center}
\ \psfig{file=figs/preclean_headroom.png,width=\textwidth}
\caption{IPC improvement over LRU when write back bandwidth costs are negated. This figure shows us idealized headroom for precleaning.}
\label{f:preclean_headroom}
\end{center}
\end{figure}

We also evaluate our basic implementation of a precleaning unit. Referring to Figure \ref{f:preclean_perf} we see little impact to performance with our implementation of a precleaning unit. While we see a small increase in performance from BH, we see a more significant decrease in performance from Rodinia's implementation of BFS. These poor results highlight the difficulty of fine tuning a predictor for precleaning. Much like prefetching, it is easy to make a precleaner too aggressive and actually hurt performance.

\begin{figure}[htb]
\begin{center}
\ \psfig{file=figs/preclean_ipc.png,width=\textwidth}
\caption{IPC improvement over LRU when cleaning the least recently used line across all sets.}
\label{f:preclean_perf}
\end{center}
\end{figure}

Our idea of precleaning certainly needs more evaluation and exploration of prediction mechanisms beyond what is provided in this report. We believe with more time and better domain knowledge we could make real performance improvements. Though it remains uncertain if these performance improvements would be significant enough to warrant the cost of an entirely new prediction unit in the L2 cache. This prediction unit could be costly as it needs to evaluate precleaning candidates, clean lines, and predict bandwidth usage on a regular basis.

We believe that, right now, precleaning isn't a viable approach for improving memory system performance on GPUs. While we can currently achieve small improvements in performance, the problem space is ill defined and the predictors will be hard to tune. If GPUs become more starved for bandwidth in the future we believe this would give us the headroom required to make precleaning worth the effort.
